{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Picture\n",
    "* Get the dataset (pairs of input and label)\n",
    "* Forward Pass: input -> function (model) -> output (prediction) \n",
    "* Compute the loss \n",
    "* Backward Pass (i.e backprop)\n",
    "* Update the parameters (i.e. weights and biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# get the dataset\n",
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH / 'mnist'\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "URL = 'https://github.com/pytorch/tutorials/raw/main/_static/'\n",
    "FILENAME = 'mnist.pkl.gz'\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "    content = requests.get(URL + FILENAME).content\n",
    "    (PATH / FILENAME).open('wb').write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), 'rb') as f:\n",
    "    # (training data), (validation data)\n",
    "    # (x is input, y is label)\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "print(\"x training data\", x_train.shape)\n",
    "print(\"y training data\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# examine the dataset\n",
    "plt.imshow(x_train[0].reshape(28, 28), cmap='gray')\n",
    "print(y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# turn numpy arrays to tensors\n",
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* numpy: scientific and numerical computing in Python\n",
    "* numpy array - multidimensional table of data - 2d, 3d, 4d...\n",
    "* when all the elements in an array is of simple type - like integer or float; numpy will store it as a compact C data structure in memory. It can run computations on the data at the same speed as optimize C code.\n",
    "* pytorch tensors are almost the same as numpy arrays - but there are some restrictions, that makes it more performant. \n",
    "* restrictions: A tensor can't be of any type - it has to be a single basic numeric type for all elements. - it also has to be rectangular in shape\n",
    "* Tensors can utilize GPUs and optimized for computation in GPUs.\n",
    "* Pytorch implements Autograd - automatically can compute gradients(derivatives) of an operation we do on tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Initialize weights and biases (i.e parameters)\n",
    "# NN Architecture: inputs -> hidden -> hidden -> output <--> label\n",
    "# Our Architecture: inputs -> output <--> label (similar to logistic regression)\n",
    "# Number of neurons in the input layer: 784(28 * 28); output layers: 10\n",
    "# (10, 784) * (784 rows, 50000 columns) -> w * x + b\n",
    "# (50000, 784) * (784, 10) -> x * w + b\n",
    "torch.manual_seed(0) # create some random numbers\n",
    "weights = torch.randn(784, 10) / math.sqrt(784) # Xavier initialization\n",
    "weights.requires_grad_() # in place apply\n",
    "bias = torch.zeros(10, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model: input -> model -> output\n",
    "# (50000, 784) -> (... * 64, 784)\n",
    "# xb -> batch of dataset\n",
    "# @ sign is an operator for matrix multiplication\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> 10 <--> 10 label(\"5\" -> [0,0,0,0,1,0,0,0,0,0])\n",
    "def softmax(x): # e^x_i / sum(e^x_j)\n",
    "    return x.exp() / x.exp().sum(-1).unsqueeze(-1)\n",
    "\n",
    "# print(softmax(model(x_train)).shape)\n",
    "\n",
    "def cross_entropy_loss(pred, targets):\n",
    "    bs, out_features = pred.shape\n",
    "    one_hot_encoded_targets = torch.eye(out_features)[targets]\n",
    "    # cross entropy loss formula: - 1/n sum(p * log(q)) = bs\n",
    "    return -(one_hot_encoded_targets * softmax(pred).log()).sum() / bs \n",
    "\n",
    "loss_func = cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# test if our implementation of cross entropy loss is correct\n",
    "bs = 64 # bs -> batch size\n",
    "xb = x_train[0:bs]\n",
    "pred = model(xb)\n",
    "yb = y_train[0:bs]\n",
    "\n",
    "print(F.cross_entropy(pred, yb))\n",
    "print(f\"loss={cross_entropy_loss(pred, yb)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_func(pred, yb):\n",
    "    pred_class = torch.argmax(pred, dim=1) # argmax\n",
    "    # pred dim -> (64, 10)\n",
    "    # [0.1, 0, 0, 0.2, 0.7, 0, 0, ...] -> 4 -> max(pred, dim=1)\n",
    "    return (pred_class == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (backward pass and parameters update)\n",
    "# Epochs -> 2\n",
    "# batches -> 50000 / 64\n",
    "\n",
    "lr = 0.5\n",
    "epochs = 2\n",
    "n = x_train.shape[0]\n",
    "num_batches = n // bs + 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(num_batches):\n",
    "        start_i = i\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        accuracy = accuracy_func(pred, yb)\n",
    "        loss.backward() # Autograd\n",
    "\n",
    "        with torch.no_grad(): # temporarily sets all of the requires_grad flag to false. to save memory consumption\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_() # otherwise, gradients will accumulate\n",
    "            bias.grad.zero_()\n",
    "\n",
    "        # Logging\n",
    "        if i % 100 == 0:\n",
    "            train_loss, train_accuracy = loss.item(), accuracy.item() * 100\n",
    "            print(f\"Loss: {train_loss:6f} Accuracy: {train_accuracy:0.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
