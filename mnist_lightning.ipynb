{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Lightning\n",
    "* It forces a standard structure on your code - easier for code review\n",
    "* It gets rid of boilerplate \n",
    "* It abstracts away alot of extra stuff besides the core code. Logging on parallelization on GPU or even TPU. Parallelization can be done just by adding some flags to your trainer rather than refactoring your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset\n",
    "Build a model\n",
    "Define loss_func and optimizer\n",
    "Define trainer\n",
    "Define test\n",
    "Run trainer and test\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "train_ds = MNIST(root='data3', train=True, download=True, transform=ToTensor())\n",
    "valid_ds = MNIST(root='data3', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "bs = 64\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchmetrics\n",
    "\n",
    "# Build the model\n",
    "class MNISTModel(pl.LightningModule): # pl.lightningmodule is an nn.module with extra features\n",
    "    def __init__(self, lr=0.5):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "        self.lr = lr\n",
    "\n",
    "        # metrics\n",
    "        # self.train_accuracy = torchmetrics.Accuracy() \n",
    "        # self.valid_accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.flatten(1, -1)\n",
    "        return self.lin(xb)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, train=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.shared_step(batch, train=False)\n",
    "\n",
    "    def shared_step(self, batch, train):\n",
    "        xb, yb = batch\n",
    "        pred = self.forward(xb)\n",
    "        loss = F.cross_entropy(pred, yb)\n",
    "\n",
    "        # Logging\n",
    "        # if train:\n",
    "        #     self.train_accuracy(pred.softmax(dim=-1), yb)\n",
    "        #     self.log('train_accuracy', self.train_accuracy, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        # else:\n",
    "        #     self.valid_accuracy(pred.softmax(dim=-1), yb)\n",
    "        #     self.log('valid_accuracy', self.valid_accuracy, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # def test_step(self, ....) -> for production use\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.SGD(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Could not get TensorBoardLogger to work. All comments related are commented out\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# logger\n",
    "# tb_logger = TensorBoardLogger('tb_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "mnist_model = MNISTModel()\n",
    "\n",
    "# init trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=2, \n",
    "    # logger=tb_logger\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.fit(mnist_model, train_dl)\n",
    "\n",
    "# optionally: run test\n",
    "# trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
